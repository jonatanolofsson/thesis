\chapter{Monocular SLAM}
\label{cha:monoslam}
    In the interest of extracting positioning information from a video stream
    of a general, unknown, environment, a common approach is to use SLAM,
    \textit{Simultaneous Localization And Mapping}, techniques.
    In the mathematical SLAM framework, features in the images are identified
    and tracked throughout time in the video stream, and a filtering solution
    is then traditionally applied to estimate the probability densities
    of the tracked landmarks' positions, as well as that of the camera position.

    To determine the depth distance to a feature, stereo vision can be applied
    with which the correlation between two synchronized images is used, together with the known
    distance between the cameras, to calculate the image depth.
    As the distance to the tracked objects increases however, the stereo
    effect is lost and the algorithms' performance drops.
    There are several other issues to the stereo vision approach - increased
    cost, synchronization issues, computational load to name a few - which has
    led to the development of techniques to utilize the information which can be extracted
    from tracking movement in only a single camera to reconstruct a three-dimensional scene.
    The application of SLAM to this approach is referred to as \textit{Monocular SLAM},
    and two approaches are presented in this chapter.

    Both approaches rely on feature detection in video frames.
    An extensive survey of available feature-detecting algorithms
    is given in \citep{idris09reviewoffeaturedetection}.

    \section{Filtering Based Solutions}
        \label{sec:video:filtersolutions}
        One novel approach for camera tracking, used by for instance \citep{DBLP:conf/iccv/Davison03} and \citep{Eade:2006:SMS:1153170.1153506},
        is to utilize a filtering framework, such as the EKF-SLAM or FastSLAM 2.0,
        and couple the feature and camera tracking in a time- and measurement
        update for each consecutive frame.
        The \textit{Scenelib} library described in \citep{DBLP:conf/iccv/Davison03}
        uses a combination of the particle filter and the EKF for feature initialization
        and feature tracking respectively.

        Common to the filter approaches is that as new features are detected,
        they are initialized and appended as states to the filter.
        As frames arrive, the change of position of each feature is measured and
        the filter is updated accordingly.
        Thus, care must be taken to avoid misassociation of the features, as
        this leads to false measurements that will mislead the filter.

        One advantage of the solutions with direct filtering is that it is quite
        trivial to extend the motion models or include other sensors to improve the precision, since
        the general algorithm utilize classical state-based time- and measurement updates.

    \section{Keyframe-based SLAM}
    \label{sec:video:kfslam}
        A fundamentally different approach, presented and used in e.g. \citep{klein07parallel},
        is to focus on collecting video frames of greater importance - keyframes -
        in which a large amount of features are detected.
        The camera's position is recorded at the time the keyframe is grabbed - see Figure~\ref{fig:video:ptamkeyframes} with caption -
        and the newly detected features are added to the active set.
        As new video frames arrive, features are reprojected and sought for in
        the new frame, giving a position offset from previously recorded keyframes which, by
        extension, gives the updated position of the camera.
        \fig{\imagewidth}{ptamkeyframes}{Keyframes containing features - here
        represented by dots - are recorded, and the camera's position at the
        time of the frame's capture is stored and visualized as coordinate axes.
        The offset from these positions is estimated from the features detected in the
        video stream.
        The thick coordinate system displays the camera's current position estimate.
        The colors of the features represent library-internal information on
        how the feature was detected.
        }{fig:video:ptamkeyframes}


\pagebreak
    \section{The PTAM Library}
        PTAM - Parallel Tracking And Mapping is a software library for camera tracking
        developed for Augmented Reality (AR) applications in small workspaces \citep{klein07parallel}.
        It has only recently been applied for quadrotor state estimation \citep{weiss11monocular},
        although as the library is intended for AR applications, the connection
        with the world's coordinate system is loose, as discussed in Chapter~\ref{cha:observer}.
        Several libraries exist extending the functionality of the PTAM library~\citep{Nguyen_Sandor_Park_2010}.

        Recently published algorithms - presented in \citep{Newcombe2011} - surpass
        the library's performance using GPGPU\footnote{General-Purpose computing on Graphics Processing Units}
        algorithms, although the performance of the PTAM library is nonetheless proven\footnote{Examples of projects using PTAM are listed at \url{http://ewokrampage.wordpress.com/projects-using-ptam/}},
        and improves the usability over preceding libraries, such as \textit{Scenelib}, by including
        a camera calibration tool, which is used to estimate the parameters describing the lens properties.
        \fig{\imagewidth}{ptamcalibration}{A checker-board pattern is used for calibrating the lens-specific parameters of the camera.}{fig:video:ptamcalibration}

        \subsection{Operation}
            The PTAM library partitions the SLAM problem into a real-time tracking
            loop and a less time-critical optimization procedure for the collected keyframes.

            In the tracking procedure, the PTAM library uses the keyframe
            technique described in Section~\ref{sec:video:kfslam}, and randomly projects
            previously recorded features into the captured video frame,
            visualized in Figure~\ref{fig:video:ptamtracking}.
            The selection of features to re-project could potentially be
            improved, as discussed in Section~\ref{sec:discussion:cameralocalization},
            although this remains as future work.
            \fig{\imagewidth}{ptamtracking}{Features - visualized as dots - are projected into the image and sought for. The offset from the positions where the features were first detected then yield a position estimate.}{fig:video:ptamtracking}

            As for mapping; Instead of continuously updating the keyframe's position estimates - as in the filtering solution -
            the PTAM library uses a less time-critical algorithm in a parallel processing thread\footnote{Hence its name; \textit{Parallel Tracking And Mapping}}.
            The applied algorithm is known as Bundle Adjustment, and
            performs a nonlinear least-squares optimization over the available keyframes
            to calibrate their in-world positions relative to each other, based on shared features~\citep{lour09}.
            The implementation utilizes the sparse structure of the optimization problem that
            is solved to make the solution computationally tractable~\citep{Lourakis11bundle}.

        \subsection{Modifications to the Library}
            The PTAM library is originally interfaced with an included graphical user interface.
            Using the source-code, which is provided free for non-commercial use, the
            library was modified to be interfaced over a serial port
            and also extended with automatic initialization and full non-graphical use.
            %~ This enables full automatization on the development platform.

            In the standard PTAM initialization procedure, an image is captured
            at a starting position. The camera is then translated sideways until the user
            deems the stereo initialization can be performed with good results.
            A second frame is then captured, and shared features are used
            to perform a stereo initialization, and extract the ground plane
            and a placement for the origin of the PTAM coordinate system.
            The tracked scene should be planar to retrieve a good estimate
            of the true ground plane, although the tracking will work regardless.

            \fig{\imagewidth}{ptamfeaturetracking}{To create the initial map, PTAM is initialized with a set of two offset images. Features are tracked during the initialization to find the ground plane. The movement of the tracked features are visualized as lines in the image.}{fig:video:ptamfeaturetracking}

            In the graphical user interface, the tracked features are visualized
            during the initialization process with lines in the camera image,
            shown in Figure~\ref{fig:video:ptamfeaturetracking}.
            After the initial frame has been captured, the proposed
            automated initialization uses the length of these lines as
            a measure of when to finalize the initialization procedure.
            When the length of the line associated with the first\footnote{First in the list of features from the first image that are still found in the latest frame.}
            feature exceeds a given threshold, the second frame is
            captured and the stereo initialization algorithm is started.
            Should the initialization procedure fail - e.g. if too many features are lost - the
            procedure will be restarted until a successful initialization has been performed.
            The procedure is initially triggered by a command from the serial port.
            The initialization is further described in Chapter~\ref{cha:logic}.

        \subsection{Practical Use}
            The PTAM library is designed to be used a wide-angle lens. It is
            documented that performance drops should such not be available~\citep{klein07parallel}.
            Even though the LinkQuad-mounted camera features a changeable lens, all currently
            available wide-angle lenses are fish-eye lenses, which the PTAM library does not currently support.
            It is possible to use the PTAM library with a standard lens, although
            tracking is less stable, and harder to extend to unexplored areas.

            The tracking and initialization quality is also - as all video tracking - dependent on the
            amount of trackable features in the scene.
            While the library can recover from lost positioning, an insufficient amount
            of features can cause the initialization process to fail, or the tracking to
            be irreparably lost.
            The amount of tracked features is also dependent on the processing
            power available, and the library may, depending on available resources,
            have difficulties extending the map to unexplored areas.
