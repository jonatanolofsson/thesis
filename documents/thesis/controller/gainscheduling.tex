\section{The State-Dependent Riccati Equation}% and LQ Gain Scheduling}
\label{sec:controller:gainscheduling}
    Even though any system could be described at any point by its linearization,
    the linear nature of the LQ control poses a limitation in that
    a general system such as the one studied in this thesis - a quadrotor - will
    sooner or later leave the vicinity of the linearization point and no
    longer adhere to the physical circumstances valid there.

    This will lead to sub-optimal control and possibly even to system failure.
    A common approach in nonlinear control is gain scheduling - to switch
    between pre-calculated control gains which has been calculated for
    selected linearization points.

    The approach used in this thesis is closely related to gain scheduling,
    but instead of using pre-calculated gains, the linearization is done
    in-flight.

    The problem of solving of the Riccati equation  online is treated under
    the subject of \textit{State-Dependent Riccati Equations}, SDRE's.
    While being computationally more expensive than the standard LQ formulation
    due to repeated solving of the Riccati equation, the need
    for finding valid linearization points is removed and more general problems can be treated.
    The basic formulation of the problem is covered in e.g. \citep{Rantzer99piecewiselinear},
    and an extensive survey is presented in \citep{Tayfun08sdresurvey}.
    Implementation details are detailed and evaluated in \citep{Erdem_analysisand,Benner98acceleratingnewton's,10.1109/MED.2006.328740}.

    The LQ theory can be extended to the nonlinear case by using the Taylor
    expansion of a general motion model, as shown in Eq.~\eqref{eq:controller:gainscheduling:xdot}.
    This approximation is exploited to form the general
    result of Eq.~\eqref{eq:controller:affinelq}.
    Similar to several other modifications to the LQ methodology - such as
    Model Predictive Control (MPC) - the cost associated with the control signal
    may be applied relative to its current level, to avoid
    forcing all control signals to zero in the optimization problem that is solved.
    In each time-step, the local linearization of the motion model, as presented in
    Eq.~\eqref{eq:controller:gainscheduling:xdot}, is used to solve the LQ-equations
    for $\Delta u$, the change in the control signal.

    \begin{equation}
        \label{eq:controller:gainscheduling:xdot}
        \dot{x} = f(x,u) \approx f(x_{0},u_{0})
            + \underbrace{\left. \frac{\partial f}{\partial x} \right|_{
                \begin{array}{l}
                    x=x_{0} \\
                    u=u_{0}
                \end{array}
            }}_{A}
                \underbrace{\left( x-x_{0} \right)}_{\Delta x}
            + \underbrace{\left. \frac{\partial f}{\partial u} \right|_{
                \begin{array}{l}
                    x=x_{0} \\
                    u=u_{0}
                \end{array}
            }}_{B}
                \underbrace{\left( u-u_{0} \right)}_{\Delta u}
    \end{equation}

    In the standard formulation of LQ control, the linearization has to be made
    at a stationary point $(x_{0},u_{0})$, where $f(x_{0},u_{0}) = 0$, to
    attain the nescessary property of linearity.
    In a more general formulation, it is possible to lift this constraint by
    using a homogeneous state, as in Eq.~\eqref{eq:controller:affinelq}
    to attain this property in the general case~\citep{Rantzer99piecewiselinear}.
    \begin{equation}
    \label{eq:controller:affinelq}
        \dot{X} = \left[
        \begin{array}{c}
            \dot{x} \\
            0
        \end{array}\right] =
        \left[
        \begin{array}{cc}
            A & f(x_{0},u_{0})-Ax_{0} \\
            0 & 0
        \end{array}\right]
        \underbrace{\left[
        \begin{array}{c}
            x \\
            1
        \end{array}\right]}_{X}
        +
        \left[
        \begin{array}{c}
            B \\
            0
        \end{array}\right]
        \Delta u
    \end{equation}
    Eq.~\eqref{eq:controller:affinelq} is a linear system for which
    the ordinary LQ problem can be solved, using Eqs.~\eqref{eq:controller:u}-\eqref{eq:controller:Lr}.
    The linearized output signal, $\Delta u$, is then added
    to $u_{0}$ to form the controller output, as in Eq.~\eqref{eq:controller:u}.
    \begin{equation}
    \label{eq:controller:u}
        u = u_{0} + \Delta u = u_{0} - L\bar{X} + L_{r}r
    \end{equation}

    A problem with the linearizing extension of the affine controller in Eq.~\eqref{eq:controller:affinelq},
    is that it introduces a non-controllable constant state, with
    an associated eigenvalue inherently located in the origin.
    This poses a problem to the traditional solvers of the Riccati equation, which
    expects negative eigenvalues to solve the problem numerically, even though
    the cost-matrices of \eqref{eq:controller:lq:j} could theoretically be chosen such as to
    obtain a well-defined, bounded, integral.

    The problem is circumvented by adding slow dynamics to the theoretically
    constant state, effectively nudging the eigenvalue to the left of the
    imaginary axis to regain full stability of the system.
    This guarantees that Eq.~\eqref{eq:controller:lq:j} tends to zero.

    Eq.~\eqref{eq:controller:affinelq} is thus implemented as in Eq.~\eqref{eq:controller:affinelqimplementation}.

    \begin{equation}
    \label{eq:controller:affinelqimplementation}
        \dot{X} = \left[
        \begin{array}{c}
            \dot{x} \\
            0
        \end{array}\right] =
        \left[
        \begin{array}{cc}
            A & f(x_{0},u_{0})-Ax_{0} \\
            0 & \mathbf{-10^{-9}}
        \end{array}\right]
        \underbrace{\left[
        \begin{array}{c}
            x \\
            1
        \end{array}\right]}_{X}
        +
        \left[
        \begin{array}{c}
            B \\
            0
        \end{array}\right]
        \Delta u
    \end{equation}
