\section{The Filtering Problem}
\label{sec:observer:filtering}
    The problem of estimating the state of a system - in this case
    it's position, orientation, velocity etc. - is in the Kalman filter
    framework expressed as the problem of finding the state estimate
    $\hat{x}$ that in a well defined best way (e.g. with Minimum Mean Square Error, MMSE)
    describes the behaviour of the system.

    The evolution of a system plant is traditionally described by a set of differential equations
    that link the change in the variables to the current state and known inputs, $u$.
    The system is also assumed to be subject to an additive white Gaussian noise $v(t)$ with
    known covariance $Q$.
    This introduces an uncertainty associated with the system, which accounts
    for imperfections in the model compared to the physical real-worl-system.
    \begin{equation}
        \label{eq:observer:xdot}
        \dot{x}(t) = f_{c}(x(t),u(t),t) + v_{c}(t)
    \end{equation}
    With numeric or analytical solutions, we can obtain the discrete form of
    \eqref{eq:observer:xdot}, where only the sampling times are considered.
    The control signal, $u(t)$, is for instance assumed to be constant in the time interval,
    and we obtain obtain the next predicted state directly, yielding the prediction of $\hat{x}$
    at the time $t$ \textit{given} the information at time $t-1$.
    \footnote{Note that we have not yet performed any measurements that provide information about the state at time t.}
    This motivates the notation used in this thesis - $\hat{x}_{t|t-1}$.
    \begin{equation}
        \label{eq:observer:xnext}
        x_{t|t-1} = f(x_{t-1|t-1},u_{t},t) + v(t)
    \end{equation}

    In the ideal case, a simulation of a prediction $\hat{x}$ would
    with the prediction model in \eqref{eq:observer:xnext} fully describe
    the evolution of the system.
    To be able to provide a good estimate in the realistic case, however,
    we must also feed back measurements given from sensors measuring
    properties of the system.

    These measurements, $y_{t}$, are fed back and fused with the
    prediction using the \textit{innovation}, $\nu$.
    \begin{equation}
        \nu_{t} = y_{t} - \hat{y}_{t}
    \end{equation}
    That is, the difference between the measured value and what would be
    expected in the ideal (simulated) case.
    To account for disturbances affecting the sensors, the measurements
    are associated with an additive white Gaussian noise $w(t)$, with
    known covariance $R$.
    \begin{equation}
        \hat{y}_{t} = h(\hat{x}_{t}, u_{t}, t) + w(t)
    \end{equation}

    The innovation is then fused with the prediction to yield a new
    estimation \citep{gustafsson2010statistical} of $x$ given the
    information available as of the time $t$.
    \begin{equation}
        \hat{x}_{t|t} = \hat{x}_{t|t-1} + K_{t}\nu_{t}
    \end{equation}

    The choice of $K_{t}$ is a balancing between of trusting the model,
    or trusting the measurement. In the Kalman filter framework,
    this balancing is made by tracking and weighing the uncertainties
    introduced by the prediction and the measurement noise.

    Because of the assumptions on the noise and the linear property
    of the innovation feedback, the Gaussian property of the noise is preserved
    in the filtering process. The system states can thus ideally
    be considered drawn from a normal distribution.
    \begin{equation}
        x \sim \mathcal{N}\left(\hat{x}, P_{xx}\right)
    \end{equation}

    Conditioned on the state and measurements before time $k$, the
    covariance of the sample distribution is defined as
    \begin{equation}
        P_{xx}(t|k) = E \left[ \left\lbrace x(t) - \hat{x}_{t|k} \right\rbrace
                               \left\lbrace x(t) - \hat{x}_{t|k} \right\rbrace^{T}
                               | \mathcal{Z}^{j} \right] .
    \end{equation}
    As new measurements are taken, the covariance of the state evolves
    \citep{Julier95anewapproach} with the state estimate as
    \begin{equation}
        P_{xx}(t|t) = P_{xx}(t|t-1) - K_{t}P_{\nu\nu}(t|t-1)K_{t}^{T}
    \end{equation}
    \begin{equation}
        \label{eq:observer:filtering:pnunu}
        P_{\nu\nu}(t|t-1) = P_{yy}(t|t-1) + R(t) .
    \end{equation}
    $K$ is derived in e.g. \citep{gustafsson2010statistical} as
    \begin{equation}
        \label{eq:observer:filtering:pxy}
        K_{t} = P_{xy}(t|t-1)P_{\nu\nu}^{-1}(t|t-1),
    \end{equation}
    given the cross-correlation of the predicted state and output, $P_{xy}$.

    There are several approaches to how to propagate the covariance
    through the prediction-model to acquire $P_{xx}(t|t-1)$ with retained Gaussian properties.
    As the linear case is simple;
    \begin{equation}
        P_{xx}(t|t-1) = AP(t|t)A^{T} + Q_{t};
    \end{equation}
    a novel approach is to linearize the system for $A$.
    This linearization, however, fails to capture the finer details
    of highly non-linear systems and may furthermore be tedious to
    calculate, analytically or otherwise. A different approach is
    discussed in Section \ref{sec:observer:ukf}.
