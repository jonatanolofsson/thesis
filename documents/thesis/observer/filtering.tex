\section{The Filtering Problem}
\label{sec:observer:filtering}
    In the general filtering problem, the estimation of the systems states - in this case
    its position, orientation, velocity etc. - is
    expressed as the problem of finding the state estimate,
    $\hat{x}$, that in a well defined best way (e.g. with Minimum Mean Square Error, MMSE)
    describes the behavior of the system.

    The evolution of a system plant is traditionally described by a set of differential equations
    that link the change in the variables to the current state and known inputs, $u$.
    This propagation through time is described by Eq.~\eqref{eq:observer:xdot} ($f_{c}$ denoting the continuous case).
    The system is also assumed to be subject to noise, $v(t)$, often assumed to
    be Gaussian and additive, with known covariance $Q$.
    This introduces an uncertainty associated with the system's state, which accounts
    for imperfections in the model compared to the physical system.
    \begin{equation}
        \label{eq:observer:xdot}
        \dot{x}(t) = f_{c}(x(t),u(t),t) + v_{c}(t)
    \end{equation}
    With numerical integration or analytical solutions, the discrete form of
    \eqref{eq:observer:xdot} is obtained, where only the sampling times are considered.
    The control signal, $u(t)$, is often assumed to be constant in the time interval,
    thereby allowing the next predicted state to be obtained straightforwardly, as in Eq.~\eqref{eq:observer:xnext}.
    This yields the estimate of $\hat{x}$ at the time $t$ \textit{given} the information at time
    $t-1$\footnote{Note that no measurements have yet been made that provide information about the state at time t.}.
    This motivates the notation used in this thesis: $\hat{x}_{t|t-1}$.
    \begin{equation}
        \label{eq:observer:xnext}
        x_{t|t-1} = f(x_{t-1|t-1},u_{t},t) + v(t)
    \end{equation}

    In the ideal case, a simulation of a prediction $\hat{x}$ would
    with the prediction model in \eqref{eq:observer:xnext} fully describe
    the evolution of the system.
    To be able to provide a good estimate in the realistic case, however,
    the measurements given from sensors measuring properties of the system
    must be fed into the estimation.

    These measurements, $y_{t}$, are fused with the
    prediction using the \textit{innovation}, $\nu$.
    \begin{equation}
        \nu_{t} = y_{t} - \hat{y}_{t}
    \end{equation}
    That is, the difference between the measured value and what would be
    expected in the ideal (predicted) case.
    To account for disturbances affecting the sensors, the measurements
    are also associated with an additive white Gaussian noise $w(t)$, with
    known covariance, $R$.
    \begin{equation}
        \hat{y}_{t} = h(\hat{x}_{t}, u_{t}, t) + w(t)
    \end{equation}

    The innovation is then fused with the prediction to yield a new
    estimation of $x$ given the
    information available at the time $t$ \citep{gustafsson2010statistical}.
    \begin{equation}
        \hat{x}_{t|t} = \hat{x}_{t|t-1} + K_{t}\nu_{t}
    \end{equation}

    The choice of $K_{t}$ is a balancing between of trusting the model,
    or trusting the measurements. In the Kalman filter framework,
    this balancing is made by tracking and weighing the uncertainties
    introduced by the prediction and the measurement noise.

    \begin{algorithm}[Kalman Filter]
        \label{alg:kf}
        For a linear system, given predictions and measurements with known respective
        covariances $Q$ and $R$, the optimal solution to the filtering
        problem is given by the forward
        recursion in Eqs.~\eqref{eq:observer:kalman:predict}-\eqref{eq:observer:kalman:measurement}\footnote{As first suggested by Rudolf E. Kálmán in \cite{kalman1960}.}.

        \begin{subequations}
            \label{eq:observer:kalman:predict}
            \textbf{Prediction update}
            \begin{align}
                \hat{x}_{t|t-1} &= A \hat{x}_{t-t|t-1} + B u_{t} \\
                P_{t|t-1} &= A P_{t-1|t-1} A^{T} + Q
            \end{align}
        \end{subequations}
        \begin{subequations}
            \label{eq:observer:kalman:measurement}
            \textbf{Measurement update}
            \begin{align}
                K &= P_{t|t-1} H^{T} \left( H P_{t|t-1} H^{T} + R \right)^{-1}  \label{eq:observer:filtering:kalmanK} \\
                \hat{x}_{t|t} &= \hat{x}_{t|t-1} + K \left( y - H \hat{x}_{t|t-1} \right) \\
                P_{t|t} &= P_{t|t-1} + K H P_{t|t-1}
            \end{align}
        \end{subequations}
    \end{algorithm}

    Because of the previously mentioned assumptions on the noise and the linear property
    of the innovation feedback, the Gaussian property of the noise is preserved
    in the filtering process. The system states can thus ideally
    be considered drawn from a normal distribution, as in Eq.~\eqref{eq:observer:xdraw}.
    \begin{equation}
        \label{eq:observer:xdraw}
        x \sim \mathcal{N}\left(\hat{x}, P_{xx}\right)
    \end{equation}

    Conditioned on the state estimate $\hat{x}$ and measurements before time $k$ ($\mathcal{Y}^{k}$), the
    covariance of the sample distribution is defined as in Eq.~\eqref{eq:observer:pxxtk}.
    \begin{equation}
        \label{eq:observer:pxxtk}
        P_{xx}(t|k) = E \left[ \left\lbrace x(t) - \hat{x}_{t|k} \right\rbrace
                               \left\lbrace x(t) - \hat{x}_{t|k} \right\rbrace^{T}
                               | \mathcal{Y}^{k} \right]
    \end{equation}
    As new measurements are taken, the covariance of the state evolves
    with the state estimate as in Eq.~\eqref{eq:observer:filtering:pxx} \citep{Julier95anewapproach}.
    \begin{align}
        P_{xx}(t|t) &= P_{xx}(t|t-1) - K_{t}P_{\nu\nu}(t|t-1)K_{t}^{T} \label{eq:observer:filtering:pxx} \\
        P_{\nu\nu}(t|t-1) &= P_{yy}(t|t-1) + R(t) \label{eq:observer:filtering:pnunu}
    \end{align}
    With known covariances, $K$ can be chosen optimally for the linear case as derived in e.g. \citep{gustafsson2010statistical}
    and given in Eq.~\eqref{eq:observer:filtering:kalmanK2}.
    \begin{equation}
        \label{eq:observer:filtering:kalmanK2}
        K_{t} = P_{xy}(t|t-1)P_{\nu\nu}^{-1}(t|t-1)
    \end{equation}
    Note that \eqref{eq:observer:filtering:kalmanK2} is another, albeit equivalent,
    way of calculating \eqref{eq:observer:filtering:kalmanK}. This is exploited
    in the derivation of the Unscented Kalman Filter, presented in Section~\ref{sec:observer:ukf}.

    Although the Kalman filter is optimal in the linear case, no guarantees
    are given for the case where the motion- or measurement model is nonlinear.
    Several methods exist to give a sub-optimal estimate for the nonlinear cases,
    two of which will be studied here.

    A major problem with the nonlinear case is how to propagate the uncertainty,
    as described by the covariance, through the prediction and measurement models.
    With the assumed Gaussian prior, it is desirable to retain the Gaussian
    property in the posterior estimate, even though this clearly is in violation with the
    nonlinear propagation, which generally does not preserve this property.

    \begin{equation}
        \label{eq:observer:filtering:pxxpredict}
        P_{xx}(t|t-1) = AP(t|t)A^{T} + Q_{t}
    \end{equation}

    As the linear propagation is simple however - shown in Eq.~\eqref{eq:observer:filtering:pxxpredict} - the
    novel approach is to linearize the system to yield the linear model $A$ in every timestep.
    This method is called the Extended Kalman Filter, and is considered the
    de facto standard nonlinear filter \cite{Julier04nonlinear}.

    \begin{algorithm}[Extended Kalman Filter]
        \label{alg:ekf}
        The Kalman filter in Algorithm~\ref{alg:kf} is in the Extended Kalman filter extended to the
        nonlinear case by straightforward linearization where necessary.

        \begin{subequations}
            \textbf{Prediction update}
            \begin{align}
                \hat{x}_{t|t-1} &= f\left( \hat{x}_{t-t|t-1}, u_{t} \right) \\
                P_{t|t-1} &= A P_{t-1|t-1} A^{T} + Q
            \end{align}
        \end{subequations}
        \begin{subequations}\textbf{Measurement update}
            \begin{align}
                K &= P_{t|t-1} H^{T} \left( H P_{t|t-1} H^{T} + R \right)^{-1} \label{eq:filtering:kalmanK} \\
                \hat{x}_{t|t} &= \hat{x}_{t|t-1} + K \left( y - h(\hat{x}_{t|t-1}) \right) \\
                P_{t|t} &= P_{t|t-1} + K H P_{t|t-1},
            \end{align}
        \end{subequations}
        where
        \begin{equation}
            \begin{array}{cc}
                A = \left.\frac{\partial f(x,u)}{\partial x}\right|_{x = \hat{x}_{t|t}}, & H = \left.\frac{\operatorname{d}\!h(x)}{\operatorname{d}\!x}\right|_{x = \hat{x}_{t|t-1}}
            \end{array}
        \end{equation}
    \end{algorithm}

    This linearization, some argue \cite{Julier95anewapproach}, fails to capture the finer details
    of highly nonlinear systems and may furthermore be tedious to
    calculate, analytically or otherwise. An alternative approach, known as the Unscented Kalman Filter, is therefore
    discussed in Section~\ref{sec:observer:ukf}.

